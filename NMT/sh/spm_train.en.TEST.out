Arguments:-
	--data = '{'/home/hatch5o6/nobackup/archive/data/LRRomance/es-en/CCMatrix/fixed/cleaned/src.100k.txt': 1.0}'
	--training_data_size = '57188'
	--save_dir = '/home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/en'
	--spm_model_name = 'en'
	--spm_vocab_size = '8000'
	--spm_model_type = 'bpe'
	--character_coverage = '1.0'
	--seed = '1500'
	--user_defined_symbols = '<pad>,<en>,<es>,<an>'
creating /home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/en
-----------------------------
data_f: /home/hatch5o6/nobackup/archive/data/LRRomance/es-en/CCMatrix/fixed/cleaned/src.100k.txt
ratio: 1.0
size: 100000
After upsampling: 57188


ALL DATA SIZE: 57188
Writing 57188 lines to /home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/en/training_data.s=1500.txt
Training tokenizer
model_name: /home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/en/en
data_file: /home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/en/training_data.s=1500.txt
model_type: bpe
vocab_size: 8000
character_coverage: 1.0
user_defined_symbols: ['<pad>', '<en>', '<es>', '<an>']
