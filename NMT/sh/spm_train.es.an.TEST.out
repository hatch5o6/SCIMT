Arguments:-
	--data = '{'/home/hatch5o6/nobackup/archive/data/LRRomance/es-an/Combined/train/train.es': .5, '/home/hatch5o6/nobackup/archive/data/LRRomance/es-an/Combined/train/train.an': .5}'
	--training_data_size = '57188'
	--save_dir = '/home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/es.an'
	--spm_model_name = 'es.an'
	--spm_vocab_size = '8000'
	--spm_model_type = 'bpe'
	--character_coverage = '1.0'
	--seed = '1500'
	--user_defined_symbols = '<pad>,<en>,<es>,<an>'
creating /home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/es.an
-----------------------------
data_f: /home/hatch5o6/nobackup/archive/data/LRRomance/es-an/Combined/train/train.es
ratio: 0.5
size: 28594
After upsampling: 28594
-----------------------------
data_f: /home/hatch5o6/nobackup/archive/data/LRRomance/es-an/Combined/train/train.an
ratio: 0.5
size: 28594
After upsampling: 28594


ALL DATA SIZE: 57188
Writing 57188 lines to /home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/es.an/training_data.s=1500.txt
Training tokenizer
model_name: /home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/es.an/es.an
data_file: /home/hatch5o6/nobackup/archive/CognateMT/spm_models/TEST/es.an/training_data.s=1500.txt
model_type: bpe
vocab_size: 8000
character_coverage: 1.0
user_defined_symbols: ['<pad>', '<en>', '<es>', '<an>']
