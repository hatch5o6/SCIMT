# outputs
src:
tgt:
save:
test_checkpoint:
remove_special_toks: True
verbose: False
little_verbose: True

# finetune?
from_pretrained: null

# data
train_data:
val_data:
test_data:
append_src_token: False
append_tgt_token: False
upsample: False

# tokenizers
spm:

# training
n_gpus: 4
seed: 1000
qos: dw87

train_batch_size: 128
val_batch_size: 128
test_batch_size: 128

early_stop: 10
save_top_k: 2
max_epochs: 1000
val_interval: 0.2 # 5 times / epoch

learning_rate: 2e-05

device: cuda

# config
encoder_layers: 6
encoder_attention_heads: 8
encoder_ffn_dim: 2048
encoder_layerdrop: 0.0

decoder_layers: 6
decoder_attention_heads: 8
decoder_ffn_dim: 2048
decoder_layerdrop: 0.0

max_position_embeddings: 512
max_length: 512
d_model: 512
dropout: 0.1
activation_function: gelu
