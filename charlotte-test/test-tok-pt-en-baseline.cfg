# Baseline Tokenizer Configuration for pt-en (NO SC augmentation)
# This tokenizer will be trained on Portuguese->English data ONLY
# Used for baseline comparison against SC-augmented model

# SPM MODELS:
SPM_TRAIN_SIZE=1600

# Source language: ONLY pt (Portuguese) - NO SC-transformed Spanish
SRC_LANGS=pt
SRC_TOK_NAME=pt

# Target language: en (English)
TGT_LANGS=en
TGT_TOK_NAME=en

# Distribution: 50% Portuguese (pt), 50% English (en) - NO Spanish
DIST=pt:50,en:50

# Training data paths (using baseline CSVs with only Portuguese-English data)
TRAIN_PARALLEL=/Users/ringger/Projects/brendan/SCIMT/charlotte-test/data/csv/nmt_train_baseline.csv
VAL_PARALLEL=/Users/ringger/Projects/brendan/SCIMT/charlotte-test/data/csv/nmt_val_baseline.csv
TEST_PARALLEL=/Users/ringger/Projects/brendan/SCIMT/charlotte-test/data/csv/nmt_test.csv

# Output directory for tokenizer training data
TOK_TRAIN_DATA_DIR=/Users/ringger/Projects/brendan/SCIMT/charlotte-test/tokenizer_data_baseline

# SC Model ID - NONE for baseline
SC_MODEL_ID=null

# Vocabulary size (same as SC-augmented for fair comparison)
VOCAB_SIZE=8000

# Tokenizer options
SPLIT_ON_WS=false
INCLUDE_LANG_TOKS=true
INCLUDE_PAD_TOK=true
SPECIAL_TOKS=null

# Is this for attention transfer? No
IS_ATT=false
