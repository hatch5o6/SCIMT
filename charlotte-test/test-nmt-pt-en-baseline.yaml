# Baseline NMT Configuration for pt->en translation WITHOUT SC augmentation
# Portuguese to English translation using ONLY Portuguese-English parallel data
# This serves as a baseline to measure the impact of CharLOTTE's SC augmentation

# Outputs
src: pt
tgt: en
save: /Users/ringger/Projects/brendan/SCIMT/charlotte-test/nmt_models/pt-en_BASELINE_s=1000
test_checkpoint: null
remove_special_toks: True
verbose: False
little_verbose: True

# Fine-tuning from pretrained (not used for quickstart)
from_pretrained: null

# Data paths (absolute paths required) - ONLY Portuguese-English data
train_data: /Users/ringger/Projects/brendan/SCIMT/charlotte-test/data/csv/nmt_train_baseline.csv
val_data: /Users/ringger/Projects/brendan/SCIMT/charlotte-test/data/csv/nmt_val_baseline.csv
test_data: /Users/ringger/Projects/brendan/SCIMT/charlotte-test/data/csv/nmt_test.csv

# Data processing - NO SC augmentation
append_src_token: False
append_tgt_token: False
upsample: False
sc_model_id: null

# Tokenizer path (without .model extension) - baseline tokenizer without SC data
spm: /Users/ringger/Projects/brendan/SCIMT/charlotte-test/spm_models/pt_en/pt_en

# Tokenizer type (false for SentencePiece, true for character-level)
do_char: false

# Training configuration (identical to SC-augmented model for fair comparison)
n_gpus: 0
seed: 1000
qos: null

train_batch_size: 16
val_batch_size: 16
test_batch_size: 16

early_stop: 10
save_top_k: 3
max_epochs: 50
max_steps: 5000
val_interval: 0.25

learning_rate: 2e-04
weight_decay: 0.01

# Device: cuda (for NVIDIA GPUs), mps (for Apple Silicon), or cpu
device: mps

# Model architecture (identical to SC-augmented model)
encoder_layers: 4
encoder_attention_heads: 4
encoder_ffn_dim: 1024
encoder_layerdrop: 0.0

decoder_layers: 4
decoder_attention_heads: 4
decoder_ffn_dim: 1024
decoder_layerdrop: 0.0

max_position_embeddings: 256
max_length: 256
d_model: 256
dropout: 0.1
activation_function: gelu
